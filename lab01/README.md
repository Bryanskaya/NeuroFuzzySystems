[Локоть](https://habr.com/ru/company/otus/blog/666376/)

[k-средних](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%85)

[k-средних](https://pythonru.com/uroki/sklearn-kmeans-i-knn)

[Локоть+k-средних](https://predictivehacks.com/k-means-elbow-method-code-for-python/)

[DBSCAN](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py)

[DBSCAN](https://www.reneshbedre.com/blog/dbscan-python.html)

[DBSCAN](https://habr.com/ru/post/322034/)

[PCA](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py)

[Почистить датасет](https://proglib.io/p/moem-dataset-rukovodstvo-po-ochistke-dannyh-v-python-2020-03-27)

[Почистить датасет](https://newtechaudit.ru/obnaruzhenie-vybrosov-v-machine-learning/)

## k-means
Используется для кластеризации данных. Выбирается число кластеров k -- начальные центры.
Вычисляются по расстроянию Евклида ближайшие к ним точки = один кластер. Затем вычисляются центроиды -- центры тяжести кластеров. 
Смещается центр кластера.

## Метод локтя
По х -- число кластеров
По у -- сумма квадратов расстрояний от каждой точки до назначенного её центра

## DBSCAN
Хорошо работает на данных, имеющих одинаковыю плотность
Требуется: е-окрестность и количество соседей

Выберем какой-нибудь корневой объект из датасета, пометим его и поместим всех его непосредственно плотно-достижимых соседей в список обхода. Теперь для каждой из списка: пометим эту точку, и, если она тоже корневая, добавим всех её соседей в список обхода. 

Тривиально доказывается, что кластеры помеченных точек, сформированные в ходе этого алгоритма максимальны (т.е. их нельзя расширить ещё одной точкой, чтобы удовлетворялись условия) и связны в смысле плотно-достижимости. Отсюда следует, что если мы обошли не все точки, можно перезапустить обход из какого-нибудь другого корневого объекта, и новый кластер не поглотит предыдущий.

## PCA (Метод главных компонент)
Один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации. 
Выбирает наиболее важные компоненты, делает ортогональными, заменяет несколько признаком одним. Получаем новую ось, также делаем вторую и вуаля.

## t-SNE
Также уменьшает размерность
Представляем расстояния в Евклидовом пространстве в условные вероятности